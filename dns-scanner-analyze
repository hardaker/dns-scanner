#!/usr/bin/python

import argparse
import hadoop_analyze
import dns_scanner_reader
import json

def parse_args():
    parser = argparse.ArgumentParser("hadoop-analyze")
    parser.add_argument("directory", metavar="directory", type=str,
                        help="Directory of files to read")
    parser.add_argument("-p", "--path", metavar="path", type=str, default="records/ttl",
                        help="Packet path to count.  EG: records/ttl or records/rrdata/ipv4_address")
    parser.add_argument("-d", "--dump", action="store_true", help="just dump what's read in")
    parser.add_argument("-j", "--json", action="store_true", help="just json about what's read in")
    return parser.parse_args()

def main():
    args = parse_args()

    # read the data
    dsr = dns_scanner_reader.DnsScannerReader()
    results = dsr.read_directory_of_files(args.directory)
    
    if args.dump:
        print results
        exit(0)

    if args.json:
        print json.dumps(results)
        exit(0)

    # analyze it
    ha = hadoop_analyze.HadoopAnalyze()
    path = args.path.split("/")
    ha.print_count_fields_at_path(results, path)

main()
